{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with 2 variables\n",
    "BSOM_data=pd.read_csv('BSOM_DataSet_for_HW2.csv',usecols = ['all_mcqs_avg_n20','all_NBME_avg_n4','LEVEL'])\n",
    "#checking for missing values\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows with missing values\n",
    "BSOM_data=BSOM_data.dropna(axis=0)\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the parameters with zeros\n",
    "def initial_parameters(size):\n",
    "    parameters=np.zeros((size,1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the probabilities\n",
    "def hypothesis(X,thetas):\n",
    "    x=np.dot(np.transpose(thetas),X)\n",
    "    h=1 / (1 + np.exp(-x))\n",
    "    zero_matrix=np.zeros(h.shape)\n",
    "    ones_matrix=np.ones(h.shape)\n",
    "    if np.array_equal(h,zero_matrix):\n",
    "        h[h==0.0]=0.00001\n",
    "     \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the cost function\n",
    "def Calc_cost(thetas,X,y):\n",
    "    h=hypothesis(X,thetas)\n",
    "    m=X.shape[1]\n",
    "    J=(-1/(m))*np.sum(y*np.log(h.astype(float))+(1-y)*np.log(1-h.astype(float)))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the gradient descent and updating the parameters\n",
    "def Gradientdescent(X,y,alpha):\n",
    "    m=X.shape[1]\n",
    "    thetas=initial_parameters(X.shape[0])\n",
    "    cost_list=[]\n",
    "    thetas_list=[]\n",
    "    thetas_list.append(thetas)\n",
    "    count=0\n",
    "    final_h=np.zeros(y.shape)\n",
    "    while True:\n",
    "        ypred=hypothesis(X,thetas)\n",
    "        cost=Calc_cost(thetas,X,y)\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if (len(cost_list)>=2) and ((cost_list[count-1]-cost_list[count])<0.00001):\n",
    "            print(\"convergence is reached at iteration\",str(count),'\\n')\n",
    "            final_h=ypred\n",
    "            break\n",
    "        update_thetas=thetas-(alpha/m)*np.matmul(X,(ypred-y).T)\n",
    "        thetas=update_thetas\n",
    "        count+=1\n",
    "    return thetas,cost_list,count,final_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the labels based on maximum probability among the 4 classifiers\n",
    "def final_prediction(h1,h2,h3,h4):\n",
    "    h1=list(h1)[0]\n",
    "    h2=list(h2)[0]\n",
    "    h3=list(h3)[0]\n",
    "    h4=list(h4)[0]\n",
    "    final_h=[]\n",
    "    max_index=[]\n",
    "    for i in range(0,len(h1)):\n",
    "        temp_list=[]\n",
    "        temp_list.append(h1[i])\n",
    "        temp_list.append(h2[i])\n",
    "        temp_list.append(h3[i])\n",
    "        temp_list.append(h4[i])\n",
    "        max_index.append(temp_list.index(max(temp_list)))\n",
    "        final_h.append(max(temp_list))\n",
    "    return final_h,max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying feature scaling to variables\n",
    "def Feature_scaling(feat):\n",
    "    num=feat.shape[0]\n",
    "    mean_X=np.mean(feat,axis=1)\n",
    "    range_X=(np.amax(feat, axis=1)-np.amin(feat, axis=1))\n",
    "    for i in range(1,num):\n",
    "        xi=feat[i,:]\n",
    "        xi=(xi-mean_X[i])/range_X[i]\n",
    "        feat[i,:]=xi\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cf_matrix):\n",
    "    sns.heatmap(cf,xticklabels=['A','B','C','D'],yticklabels=['A','B','C','D'],annot=True,linecolor='white',linewidths=0.5,cmap='coolwarm')\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"actual labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train(70%) and test(30%) datasets\n",
    "features_X = BSOM_data.iloc[:,:-1].to_numpy()\n",
    "y=BSOM_data.iloc[:,-1].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(features_X, y, test_size = 0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding bias terms and feature scaling\n",
    "train_X = Xtrain\n",
    "m_train=train_X.shape[0]\n",
    "train_X=np.append(np.ones((m_train,1)),train_X,axis=1).T\n",
    "train_y=ytrain\n",
    "train_y=pd.get_dummies(train_y).to_numpy()\n",
    "train_X=Feature_scaling(train_X)\n",
    "test_X = Xtest\n",
    "m_test=test_X.shape[0]\n",
    "test_X=np.append(np.ones((m_test,1)),test_X,axis=1).T\n",
    "test_y=ytest\n",
    "test_y=pd.get_dummies(test_y).to_numpy()\n",
    "test_X=Feature_scaling(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the class labels in the train data\n",
    "actual_train=ytrain\n",
    "actual_train=np.where(actual_train=='A', 0, actual_train)\n",
    "actual_train=np.where(actual_train=='B', 1, actual_train)\n",
    "actual_train=np.where(actual_train=='C', 2, actual_train)\n",
    "actual_train=np.where(actual_train=='D', 3, actual_train)\n",
    "#encode the class labels in the test data\n",
    "actual_test=ytest\n",
    "actual_test=np.where(actual_test=='A', 0, actual_test)\n",
    "actual_test=np.where(actual_test=='B', 1, actual_test)\n",
    "actual_test=np.where(actual_test=='C', 2, actual_test)\n",
    "actual_test=np.where(actual_test=='D', 3, actual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##best parameters with best alpha on training data set\n",
    "best_alpha=0.6\n",
    "print(\"classifier 1(class A vs not class A)\")\n",
    "best_coef1,costs_J1,num_iter1,best_train_pred1=Gradientdescent(train_X,train_y[:,0],best_alpha)\n",
    "print(\"classifier 2(class B vs not class B)\")\n",
    "best_coef2,costs_J2,num_iter2,best_train_pred2=Gradientdescent(train_X,train_y[:,1],best_alpha)\n",
    "print(\"classifier 3(class C vs not class C)\")\n",
    "best_coef3,costs_J3,num_iter3,best_train_pred3=Gradientdescent(train_X,train_y[:,2],best_alpha)\n",
    "print(\"classifier 4(class D vs not class D)\")\n",
    "best_coef4,costs_J4,num_iter4,best_train_pred4=Gradientdescent(train_X,train_y[:,3],best_alpha)\n",
    "best_pred,best_labels=final_prediction(best_train_pred1,best_train_pred2,best_train_pred3,best_train_pred4)\n",
    "final_labels_train=np.array(best_labels)\n",
    "print(\"Confusion Matrix \\n\")\n",
    "cf=confusion_matrix(list(actual_train),list(final_labels_train))\n",
    "print(cf)\n",
    "pr=precision_score(list(actual_train),list(final_labels_train),average='macro')\n",
    "rc=recall_score(list(actual_train),list(final_labels_train),average='macro')\n",
    "f1=f1_score(list(actual_train),list(final_labels_train),average='macro')\n",
    "print(\"Precision : \",str(pr))\n",
    "print(\"Recall : \",str(rc))\n",
    "print(\"F1 score : \",str(f1))\n",
    "print(\"Confusion Matrix of training data\")\n",
    "plot_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on test data with best alpha and best parameters\n",
    "test_pred1=hypothesis(test_X,best_coef1)\n",
    "test_pred2=hypothesis(test_X,best_coef2)\n",
    "test_pred3=hypothesis(test_X,best_coef3)\n",
    "test_pred4=hypothesis(test_X,best_coef4)\n",
    "pred_test,labels_test=final_prediction(test_pred1,test_pred2,test_pred3,test_pred4)\n",
    "final_labels_test=np.array(labels_test)\n",
    "print(\"Confusion Matrix \\n\")\n",
    "cf=confusion_matrix(list(actual_test),list(final_labels_test))\n",
    "print(cf)\n",
    "pr=precision_score(list(actual_test),list(final_labels_test),average='macro')\n",
    "rc=recall_score(list(actual_test),list(final_labels_test),average='macro')\n",
    "f1=f1_score(list(actual_test),list(final_labels_test),average='macro')\n",
    "print(\"Precision : \",str(pr))\n",
    "print(\"Recall : \",str(rc))\n",
    "print(\"F1 score : \",str(f1))\n",
    "print(\"Confusion Matrix of test data\")\n",
    "plot_confusion_matrix(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
