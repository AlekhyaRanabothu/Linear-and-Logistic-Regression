{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with one variable\n",
    "BSOM_data=pd.read_csv('BSOM_DataSet_for_HW2.csv',usecols = ['all_mcqs_avg_n20','STEP_1'])\n",
    "#checking for missing values\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling misssing values\n",
    "BSOM_data['STEP_1']=BSOM_data['STEP_1'].fillna(BSOM_data['STEP_1'].mean())\n",
    "BSOM_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train(80%) and test(20%) datasets\n",
    "features_X = BSOM_data.iloc[:,:-1].to_numpy()\n",
    "y=BSOM_data.iloc[:,-1].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(features_X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot features and actual y values\n",
    "x1=Xtrain\n",
    "plt.scatter(x1,ytrain)\n",
    "plt.xlabel(\"mcqs_avg\")\n",
    "plt.ylabel(\"STEP_1\")\n",
    "plt.title(\"LinearRegression with One variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and test features,training and test target,number of examples of training and test data\n",
    "train_X = Xtrain\n",
    "m_train=train_X.shape[0]\n",
    "#add bias term for training features\n",
    "train_X=np.append(np.ones((m_train,1)),train_X,axis=1).T\n",
    "test_X = Xtest\n",
    "m_test=test_X.shape[0]\n",
    "#add bias term for test features\n",
    "test_X=np.append(np.ones((m_test,1)),test_X,axis=1).T\n",
    "train_y=ytrain\n",
    "train_y=train_y.reshape(1,train_y.shape[0])\n",
    "\n",
    "test_y=ytest\n",
    "test_y=test_y.reshape(1,test_y.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the parameters with zeros\n",
    "def initial_parameters(size):\n",
    "    parameters=np.zeros((size,1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the prediction\n",
    "def hypothesis(X,thetas):\n",
    "    h=np.dot(np.transpose(thetas),X)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the cost function\n",
    "def calc_cost(thetas,X,y):\n",
    "    h=hypothesis(X,thetas)\n",
    "    m=X.shape[1]\n",
    "    J=(1/(2*m))*np.sum((h-y)**2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the gradient descent\n",
    "def Gradientdescent(X,y,alpha):\n",
    "    m=X.shape[1]\n",
    "    thetas=initial_parameters(X.shape[0])\n",
    "    cost_list=[]\n",
    "    thetas_list=[]\n",
    "    thetas_list.append(thetas)\n",
    "    iterations=0\n",
    "    while True:\n",
    "        ypred=hypothesis(X,thetas)\n",
    "        cost=calc_cost(thetas,X,y)\n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if (len(cost_list)>=2) and (cost_list[iterations-1]==cost_list[iterations]):\n",
    "            print(\"convergence is reached at iteration  \",str(iterations),\"when alpha is: \",str(alpha))\n",
    "            print(\"minimum cost is :\",str(cost))\n",
    "            \n",
    "            break\n",
    "        update_thetas=thetas-(alpha/m)*np.matmul(X,(ypred-y).T)\n",
    "        thetas=update_thetas\n",
    "        iterations+=1\n",
    "        \n",
    "    return thetas,cost_list,iterations,ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model using the train dataset\n",
    "alphas_list=[0.1,0.5,0.6,0.7]\n",
    "for i in alphas_list:\n",
    "    print(\"learning_rate :\",str(i))\n",
    "    bestcoef,J_list,iter_num,train_pred=Gradientdescent(train_X,train_y,i)\n",
    "    print(\"best parameters \",str(bestcoef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent with good learning rate\n",
    "#best parameters with best alpha\n",
    "bestcoef_train,J_list_train,iter_num_train,train_pred_1=Gradientdescent(train_X,train_y,0.7)\n",
    "print(\"best thetas : \",str(bestcoef_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the cost (vs) iterations graph\n",
    "iterations=list(np.arange(0,iter_num,500))\n",
    "cost_J=[]\n",
    "for i in iterations:\n",
    "    cost_J.append(J_list[i])\n",
    "\n",
    "plt.plot(iterations,cost_J)\n",
    "plt.xlabel(\"#Iterations\")\n",
    "plt.ylabel(\"J (cost)\")\n",
    "plt.title(\"LinearRegression with One variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the prediction of train dataset with best parameters and learning rate\n",
    "x1=Xtrain\n",
    "plt.scatter(x1,ytrain)\n",
    "plt.plot(x1,train_pred_1.T, color='red')\n",
    "plt.xlabel(\"mcqs_avg\")\n",
    "plt.ylabel(\"STEP_1\")\n",
    "plt.title(\"Prediction on train dataset with One variable\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting using test set with best parameters and learning rate\n",
    "test_pred=hypothesis(test_X,bestcoef_train)\n",
    "#test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the prediction of test dataset with best parameters and learning rate\n",
    "\n",
    "xt=Xtest\n",
    "plt.scatter(xt,ytest)\n",
    "plt.plot(xt,test_pred.T, color='red')\n",
    "plt.xlabel(\"mcqs_avg\")\n",
    "plt.ylabel(\"STEP_1\")\n",
    "plt.title(\"Prediction on test set with One variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate Mean squared error\n",
    "def meansquarederror(actual,predicted):\n",
    "    m=actual.shape[1]\n",
    "    MSE=(1/m)*np.sum((predicted-actual)**2)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate R squared\n",
    "def Rsquared(actual,predicted):\n",
    "    y_mean=np.mean(actual)\n",
    "    SSE=np.sum((actual-predicted)**2)\n",
    "    SST=np.sum((actual-y_mean)**2)\n",
    "    Rsquare=1-(SSE/SST)\n",
    "    return Rsquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Pearson correlation coefficient\n",
    "def Correlation_coef(actual,predicted):\n",
    "    y_mean=np.mean(actual)\n",
    "    x_mean=np.mean(predicted)\n",
    "    numerator=np.sum((actual-y_mean)*(predicted-x_mean))\n",
    "    denominator=np.sqrt(np.sum((actual-y_mean)**2)*np.sum((predicted-x_mean)**2))\n",
    "    pc=numerator/denominator\n",
    "    return pc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R2 score of training data with one variable\n",
    "rs1=Rsquared(train_y,train_pred_1)\n",
    "print(\"R2 score of training data :\",str(rs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Mean Squared Error of training data with one variable\n",
    "ms1=meansquarederror(train_y,train_pred_1)\n",
    "print(\"MSE of training data : \",str(ms1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Pearson correlation coefficient of training data with one variable\n",
    "pc1=Correlation_coef(train_y,train_pred_1)\n",
    "print(\"correlation coefficient on training data : \",str(pc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R2 score of test data with one variable\n",
    "rs2=Rsquared(test_y,test_pred)\n",
    "print(\"R2 score of test data :\",str(rs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Mean Squared Error of test data with one variable\n",
    "ms2=meansquarederror(test_y,test_pred)\n",
    "print(\"MSE of test data : \",str(ms2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Pearson correlation coefficient of test data with one variable\n",
    "pc2=Correlation_coef(test_y,test_pred)\n",
    "print(\"correlation coefficient on training data : \",str(pc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset with two variable\n",
    "BSOM_data_2=pd.read_csv('BSOM_DataSet_for_HW2.csv',usecols = ['all_mcqs_avg_n20','all_NBME_avg_n4','STEP_1'])\n",
    "#checking for missing values\n",
    "BSOM_data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling misssing values\n",
    "BSOM_data_2['STEP_1']=BSOM_data_2['STEP_1'].fillna(BSOM_data_2['STEP_1'].mean())\n",
    "BSOM_data_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train(80%) and test(20%) datasets\n",
    "features_X_2 = BSOM_data_2.iloc[:,:-1].to_numpy()\n",
    "y_2=BSOM_data_2.iloc[:,-1].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain_2, Xtest_2, ytrain_2, ytest_2 = train_test_split(features_X_2, y_2, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and test features,training and test target,number of examples of training and test data\n",
    "train_X_2 = Xtrain_2\n",
    "m_train_2=train_X_2.shape[0]\n",
    "#adding bias term for features in training data\n",
    "train_X_2=np.append(np.ones((m_train_2,1)),train_X_2,axis=1).T\n",
    "test_X_2 = Xtest_2\n",
    "m_test_2=test_X_2.shape[0]\n",
    "#adding bias term for features in training data\n",
    "test_X_2=np.append(np.ones((m_test_2,1)),test_X_2,axis=1).T\n",
    "train_y_2=ytrain_2\n",
    "train_y_2=train_y_2.reshape(1,train_y_2.shape[0])\n",
    "\n",
    "test_y_2=ytest_2\n",
    "test_y_2=test_y_2.reshape(1,test_y_2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model using the train dataset\n",
    "alphas_list=[0.1,0.5,0.6,0.7]\n",
    "for i in alphas_list:\n",
    "    print(\"learning_rate :\",str(i))\n",
    "    coef,costs_J,num_iter,train_pred_2v=Gradientdescent(train_X_2,train_y_2,i)\n",
    "    print(\"best parameters \",str(coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters with best alpha\n",
    "bestcoef_train2,J_list_train2,iter_num_train2,train_pred_2=Gradientdescent(train_X_2,train_y_2,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the cost (vs) iterations graph\n",
    "iterations=list(np.arange(0,num_iter,10000))\n",
    "\n",
    "cost_J=[]\n",
    "for i in iterations:\n",
    "    cost_J.append(costs_J[i])\n",
    "\n",
    "plt.plot(iterations,cost_J)\n",
    "plt.xlabel(\"#Iterations\")\n",
    "plt.ylabel(\"J (cost)\")\n",
    "plt.title(\"LinearRegression with 2 variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting using test set with best parameters and learning rate\n",
    "test_pred_2=hypothesis(test_X_2,bestcoef_train2)\n",
    "#test_pred_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R2 score of training data with two variables\n",
    "rs2v=Rsquared(train_y_2,train_pred_2)\n",
    "print(\"R2 score of training data :\",str(rs2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Mean Squared Error of training data with 2 variables\n",
    "ms2v=meansquarederror(train_y_2,train_pred_2)\n",
    "print(\"MSE of training data : \",str(ms2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Pearson correlation coefficient of training data with 2 variables\n",
    "pc2v=Correlation_coef(train_y_2,train_pred_2)\n",
    "print(\"correlation coefficient on training data : \",str(pc2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate R2 score of test data with 2 variables\n",
    "rstest2=Rsquared(test_y_2,test_pred_2)\n",
    "print(\"R2 score of test data :\",str(rstest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Mean Squared Error of test data with 2 variables\n",
    "ms2test=meansquarederror(test_y_2,test_pred_2)\n",
    "print(\"MSE of test data : \",str(ms2test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Pearson correlation coefficient of test data with 2 variables\n",
    "pc2v_test=Correlation_coef(test_y_2,test_pred_2)\n",
    "print(\"correlation coefficient on training data : \",str(pc2v_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
